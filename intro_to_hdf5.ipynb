{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 + Python\n",
    "\n",
    "Let's say you have a giant array that you need to work with. So giant, in fact, that it is bigger than your computer's temporary memory. What can we do? \n",
    "\n",
    "## HDF5 Background\n",
    "\n",
    "HDF5 (Hierarchical Data Format) is a file format that can handle your data, no matter how big your data are. There are packages for working with HDF5 files in most languages. HDF5 [has been suggested](https://arxiv.org/abs/1411.0507) as a possible replacement for FITS files in the astronomy community, but there's enough inertia that it's probable that this is the first time you'll hear about HDF5.\n",
    "\n",
    "The Python package for working with HDF5 files is `h5py` ([docs](http://docs.h5py.org/en/latest/index.html)).\n",
    "\n",
    "## First example\n",
    "\n",
    "Let's say you have a 3D numpy array that you want to store. Let's make an array with shape `(10, 10, 10)` to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2019)\n",
    "\n",
    "data = np.random.randn(1000).reshape((10, 10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could save this in a FITS file, you could save this as a numpy pickle file. We're going to save it with HDF5. Starting a new file with HDF5 is a little cumbersome, but let's go for it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py   # This is the python interface to HDF5\n",
    "\n",
    "f = h5py.File('data.hdf5', 'w')  # 'w' indicates we are creating this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An HDF5 file contains _datasets_. A dataset is an array with one data type. Let's create one for `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = f.create_dataset('dataset', shape=data.shape, dtype=data.dtype)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fill a dataset with data, you need to take a slice of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[:] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `dataset` can be sliced like a numpy array (with some caveats), and the arrays returned will be numpy arrays. The trick going on behind the scenes is that `h5py` will only access the memory for the parts of the data cube that you slice for. This is powerful because your data cube could contain 10 GB of information on a machine with 2 GB of RAM, but if you try to access only `dataset[0, 0, :]`, you'll only read that slice of the data cube from memory.\n",
    "\n",
    "Let's do a quick computation on a slice of the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = dataset[0, :, :]\n",
    "\n",
    "image.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for datasets with high dimensions, you can use the ellipsis (`...`) syntax for specifying \"give me all of the values from all of the remaining dimensions\", like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This: \n",
    "image = dataset[0, :, :]\n",
    "\n",
    "# is equivalent to this: \n",
    "image = dataset[0, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're done with a file, we should close the file stream: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big(ger) Data Example\n",
    "\n",
    "The above example used a small dataset to demonstrate the functionality of `h5py` without taking up all of your memory. If you have plenty of disk space (0.6 GB), you may procede with caution to the example below. \n",
    "\n",
    "**Warning**: you should only do the example below if you have sufficient disk space to store a 0.6 GB file.\n",
    "\n",
    "We will create a really big dataset, and use one of the [HDF5 compression types](http://docs.h5py.org/en/latest/high/dataset.html#lossless-compression-filters) to store it on your machine succinctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sufficient_disk_space = False\n",
    "\n",
    "if sufficient_disk_space:\n",
    "    f = h5py.File('big_data.hdf5', 'w')\n",
    "    \n",
    "    # Set image dimensions, number of images\n",
    "    image_dimensions = (2048, 2048)\n",
    "    number_images = 20\n",
    "    \n",
    "    # Create a dataset, and specify that it will be compressed with the LZF algorithm \n",
    "    images = f.create_dataset('images', dtype=np.float64, compression='lzf',\n",
    "                              shape=(number_images, image_dimensions[0], image_dimensions[1]))\n",
    "    \n",
    "    # Create random images, and add them to the HDF5 archive\n",
    "    for i in range(number_images):\n",
    "        n_pixels = image_dimensions[0] * image_dimensions[1]\n",
    "        random_image = np.random.randn(n_pixels).reshape(image_dimensions)\n",
    "        \n",
    "        images[i, ...] = random_image\n",
    "        \n",
    "    # Close the file stream\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do the math, this is how large we'd expect that file to be without compression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if sufficient_disk_space:\n",
    "    print(number_images * image_dimensions[0] * image_dimensions[1] * 64 / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the actual size of the file is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -lh big_data.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LZF` compression algorithm is very fast, but not the most compact (try `gzip` for an alternative which is slower but compresses the file a bit more).\n",
    "\n",
    "Now let's access the data in that HDF5 archive: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if sufficient_disk_space:\n",
    "    f = h5py.File('big_data.hdf5', 'r')\n",
    "\n",
    "    # access the images dataset\n",
    "    images = f['images']\n",
    "    \n",
    "    # Get the first image from the dataset:\n",
    "    first_image = images[0, ...]\n",
    "    \n",
    "    plt.imshow(first_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice - we didn't have to wait for the whole file to load into memory, and we didn't have to specify the compression to decompress it - we just asked for the slice of data that we wanted, and we got it! That's because the `images` variable is not holding the whole numpy array, it's holding the `h5py` HDF5 dataset object, which behaves like a numpy array without actually being one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if sufficient_disk_space:\n",
    "    print(images)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "\n",
    "Open your operating system's activity monitor, and look at how much memory is being used currently. \n",
    "\n",
    "Now in the cell below: \n",
    "1. Open the HDF5 archive `big_data` that we created earlier. \n",
    "2. Write a loop that measures the standard deviation of the values in each image\n",
    "3. Check that the whole data cube isn't being accessed using your system's activity monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
